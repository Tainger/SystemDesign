## 第六章

键值存储，也称为键值数据库，是一个非关系数据库。 每个唯一标识符都存储为具有其关联值的键。 该数据配对称为“键值”对。 

在键值对中，密钥必须是唯一的，并且可以通过密钥访问与密钥相关联的值。 键可以是纯文本或散列值。 出于性能原因，短键工作更好。 钥匙看起来像什么？ 

以下是一些示例：
- 纯文本键：“last_logging_in_in”
- 哈希密钥：253ddec4
  
键值对中的值可以是字符串，列表，对象等。值通常被视为键值中的不透明对象 商店，如 Amazon dynamo[1]，memcached [2]，redis [3]等 

这是键值存储中的数据片段:

| key   | value   |
| :------: |:------:|
| 145 | john |
| 147 | bob |
| 160 | Julia |

在本章中，要求您设计支持以下操作的键值存储： 
- 放置（键，值）//插入与“键”关联的“值”
- GET（键）//获取“值”关联 用“钥匙 

### 了解问题并建立设计范围 

没有完美的设计。 每种设计都会在关读，写和内存使用的取一个的特定平衡。 还要在一致性和可用性之间取一个平衡。 
在本章中，我们设计了一个键值存储，包括以下特征：

- 键值对的大小小：小于10 kB。
- 存储大数据的能力。
- 高可用性：即使在故障期间，系统也会快速响应。 
- 高可伸缩性：可以缩放系统以支持大数据集。 
- 自动缩放：服务器的添加/删除应基于流量自动。 
- 可调一致性。
- 低延迟 


### 单服务器键值存储 

开发驻留在单个服务器中的密钥值存储很容易。 
直观的方法是将钥匙值对存储在哈希表中，这将所有内存中的所有内容保持在内。 
尽管存储器访问快速，但由于空间约束，可能不可能拟合在存储器中的所有内容。 
可以完成两个优化以在单个服务器中拟合更多数据:
- 数据压缩
- 仅存储在内存中的常用数据以及磁盘上的其余数据 

即使使用这些优化，单个服务器也可以很快达到其容量限制。 需要一个分布式键值存储来支持大数据。

### 分布式键值存储

分布式键值存储器也称为分布式哈希表，该表在许多服务器上分发了键值对。 
在设计分布式系统时，了解帽（一致性，可用性，分区容差）定理非常重要。

#### cap理论
CAP定理说明分布式系统不可能同时提供这三个保证中的两种以上：一致性，可用性和分区容差。 
让我们建立一些定义

**一致性:** 一致性意味着无论客户端连接到哪个节点，所有客户端都可以同时看到相同的数据。
**高可用:** 可用性意味着即使某些节点的服务器宕机，请求数据的任何客户端都会获得响应。
**分区容忍性:** 分区表示两个节点之间的通信中断，除了网络分区，分区公差意味着系统继续运行。

一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。

当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。

提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。

然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。

总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。

CAP定理指出，必须牺牲三个属性中的一个，以支持3个属性的2，如图6-1所示 

**CP（一致性和分区容差）系统**：CP键值存储支持一致性和分区容差，同时牺牲可用性。 

**AP（可用性和分区公差）系统**：AP键值存储支持可用性和分区容差，同时牺牲一致性。 

**CA（一致性和可用性）系统**：CA键值存储支持一致性和可用性，同时牺牲分区公差。 由于网络故障是不可避免的，分布式系统必须容忍网络分区。 因此，CA系统不能存在于实际应用中

#### 举例

你上面读的是主要是定义部分。 为了让它更容易理解，让我们看看一些具体的例子。 在分布式系统中，数据通常复制多次。 假设数据在三个副本节点，n1，n2和n3上复制数据，如图6-2所示 

##### 理想的世界

在理想的世界中，网络分区永远不会发生。 写入N1的数据自动复制到N2和N3。 实现一致性和可用性

![img.png](/Users/rocky/study/github/SystemDesign/image/img.png)
图 6-2
##### 真实的分布式系统

在分布式系统中，无法避免分区，并且当发生分区时，我们必须在一致性和可用性之间进行选择。 
在图6-3中，N3下降并无法与N1和N2通信。 如果客户端将数据写入N1或N2，则数据不能将数据传播到N3。
如果数据被写入N3但未传播到N1和N2，则N1和N2将具有脏数据 

![img.png](/Users/rocky/study/github/SystemDesign/image/img_1.png)
图 6-3
如果我们选择一致性而不是可用性（CP系统），则必须阻止对n1和n2的所有写操作，以避免这三台服务器之间的数据不一致，从而使系统不可用。 银行系统通常具有极高的一致性要求。
例如，对于银行系统来说，显示最新的余额信息至关重要。 如果由于网络分区而发生不一致，则银行系统将在解决不一致之前返回错误。

但是，如果我们选择过度的一致性（AP系统），则系统将继续接受读取，即使它可能会返回陈旧数据。 
对于Writes，N1和N2将继续接受写入，并且当网络分区解决时，数据将同步到N3。

选择正确的cap理论是保证符符合你的应用场景，是构建分布式键值商店的一个重要步骤。 您可以通过面试官讨论此问题并相应地设计系统。

##### 系统组件
在本节中，我们将讨论以下核心组件和技术，用于构建密钥值存储:
- 数据分区
- 数据副本
- 一致性
- 非一致性解决方案
- 处理失败的策略
- 系统架构图
- 写路径
- 读路径

以下内容主要基于三个流行的键值存储系统：Dynamo [4]，Cassandra [5]和Bigtable [6]

##### 数据分区

对于大型应用程序，将完整的数据设置在单个服务器中的完整数据不可行。 实现这一目标的最简单方法是将数据拆分为较小的分区并将其存储在多个服务器中。 分区数据时有两个挑战:

- 将资源均匀的分布在多台服务器上。
- 在添加或删除节点时最小化数据移动。 

第5章中讨论的一致散列是解决这些问题的伟大技巧。 让我们重新看下哈希一致性算法并且利用它进行高水平设计。

- 首先，服务器放在哈希环上。 在图6-4中，由S0，S1，...，S7表示的八个服务器放置在哈希环上。
- 接下来，将钥匙散列到同一环上，并且存储在沿顺时针方向移动的第一服务器上。 例如，key0使用此逻辑存储在S1中。
  ![img.png](/Users/rocky/study/github/SystemDesign/image/img_2.png)
  
使用哈希一致性算法来分区数据具有以下优点:

- 自动扩容: 根据负载，可以自动添加和删除服务器
- 异质性: 服务器的虚拟节点的数量与服务器容量成比例。 例如，具有更多虚拟节点的容量具有更高容量的服务器 

##### 数据副本

为了实现高可用性和可靠性，必须在N服务器上异步复制数据，其中n是可配置参数。 
使用以下逻辑选择这些N服务器：键映射到散列圈上的位置后，从该位置顺时针行走，然后在环上选择第一个n服务器以存储数据副本。 在图6-5中（n = 3），key0在s1，s2和s3处复制。
![img.png](/Users/rocky/study/github/SystemDesign/image/6-5.png)

利用虚拟节点，环上的第一个N个节点可能由少于N个物理服务器拥有。 为避免此问题，我们只选择唯一的服务器，同时执行顺时针步行逻辑。 
同一数据中心中的节点通常由于停电，网络问题，自然灾害等而失败。为了更好的可靠性，副本被放置在不同的数据中心，通过高速网络连接数据中心。

##### 一致性
由于数据在多个节点处复制，因此它必须跨副本同步。
Quorum共识可以保证读写操作的一致性。 让我们首先建立一些定义。

n =副本的数量
W =写仲裁的尺寸W。 对于要被视为成功的写操作，必须从W副本确认写入操作。 
r =读仲裁的尺寸R。 对于读取操作被视为成功，读取操作必须等待至少r副本的响应。

请考虑以下示例，图6-6中显示为n = 3
![img.png](/Users/rocky/study/github/SystemDesign/image/6-6.png)
W = 1并不意味着将数据写入一台服务器。 例如，使用图6-6中的配置，数据将复制到s0，s1和s2。 
W = 1表示协调器必须至少收到一个确认，然后才能将写入操作视为成功。 例如，如果我们从s1获得确认，则不再需要等待s0和s2的确认。 协调器充当客户端和节点之间的代理。

W，R和N的配置是延迟和一致性之间的典型权衡。 如果w = 1或r = 1，则快速返回操作，因为协调器只需要等待来自任何副本的响应。 
如果W或R> 1，系统提供更好的一致性; 但是，查询将慢得慢，因为协调器必须等待从最慢的副本中的响应

如果W + R> N，则保证强的一致性，因为必须至少有一个重叠节点具有最新的数据，以确保一致性 

如何配置N，W和R以适合我们的用例？ 
以下是一些可能的设置：如果R = 1且W = N，则系统已针对快速读取进行了优化。 如果W = 1且R = N，则系统针对快速写入进行了优化。 
如果W + R> N，则可以保证强一致性（通常N = 3，W = R = 2）。 如果W + R <= N，则不能保证强一致性。

根据要求，我们可以调整w，r，n的值来达到所需的一致性水平。

######一致性模型
一致性模型是设计键值存储时需要考虑的其他重要因素。 一致性模型定义了数据一致性程度，并且存在广泛的可能一致性模型:

- 强一致性：任何读取操作都返回对应于最新的写入数据项的结果的值。 客户端从未看到过时的数据 
- 弱一致性：后续读取操作可能看不到最新的值 
- 最终一致性：这是一种弱良好的特定形式。 给予足够的时间，所有更新都传播，并且所有副本都是一致的。

通常，通过强制副本在每个副本都同意当前写入之前不接受新的读/写来实现强一致性。 这种方法对于高可用性系统不是理想的，因为它可能会阻止新操作。 
Dynamo和Cassandra采用最终一致性，这是我们为键值存储推荐的一致性模型。 通过并发写入，最终的一致性允许不一致的值进入系统，并迫使客户端读取要协调的值。 
下一节将说明对帐如何与版本控制一起使用。

######非一致性解决方案

##### 处理失败

与任何大型系统一样，失败不仅是不可避免的而且常见的。 处理失败情景非常重要。 在本节中，我们首先介绍检测失败的技术。 
然后，我们越过常见的失败解决策略。

###### 失败监测

在分布式系统中，它不足以相信服务器已关闭，因为另一台服务器如此。 通常，它需要至少两个独立的信息来源来标记服务器。

如图6-10所示，所有组播都是一种简单的解决方案。 但是，当系统中有许多服务器时，这效率很低。
![img.png](/Users/rocky/study/github/SystemDesign/image/6-10.png)

更好的解决方案是使用像Gossip协议等分散的故障检测方法。 八卦协议如下工作:

- 每个节点都维护一个节点成员资格列表，其中包含成员ID和心跳计数器.
- 每个节点会定期增加其心跳计数器。
- 一旦节点接收心跳，会员列表将更新到最新信息
- 如果心跳没有增加超过预定义的时间，则会被视为离线


![img.png](/Users/rocky/study/github/SystemDesign/image/6-11.png)

- 节点S0维护左侧显示的节点成员资格列表。
- 节点s0注意到节点s2的（成员ID = 2）心跳计数器长时间未增加。
- 节点S0将包含S2的信息的心跳发送到一组随机节点。 一旦其他节点确认已经长时间尚未更新S2的心跳计数器，节点S2被标记为下线了，并且此信息将传播到其他节点。 

###### 处理短暂的失败

通过Gossip协议检测到故障后，系统需要部署某些机制以确保可用性。 在严格的仲裁方法中，可以阻止读写操作，如仲裁共识部分所示:

一种称为“草率仲裁”的技术[4]用于提高可用性。 系统不执行强制仲裁要求，而是在散列环上选择前W个运行状况良好的服务器进行写入，并选择前R个运行状况良好的服务器进行读取。 脱机服务器将被忽略。

如果由于网络或服务器故障导致服务器不可用，则另一台服务器将临时处理请求。 当下线服务器启动时，将改变的数据给下线的服务器以实现数据一致性。 此过程称为暗示切换。 由于S2在图6-12中不可用，因此S3暂时处理读取和写入。 当S2返回离线时，S3会将数据交回S2 

###### 处理永久的失败

暗示的切换用于处理临时故障。 如果复制品永久性不可用何时何地？ 为了处理这种情况，我们实施了一个反熵协议，以保持复制品同步。 反熵涉及将每个数据上的数据进行比较，并将每个副本更新到最新版本。 Merkle树用于不一致检测并最大限度地减少传输的数据量。

从维基百科引用[7]：“哈希树或Merkle树是一个树，其中每个非叶节点都标有其子节点的标签或值的散列（如叶子）。 哈希树允许高效安全地验证大数据结构的内容。


###### 处理数据中心中断
由于停电，网络中断，自然灾害等，建立一个能够处理数据中心中断的系统可能会发生数据中心中断，以多个数据中心复制数据非常重要。 
即使数据中心完全脱机，用户仍然可以通过其他数据中心访问数据。

###### 系统架构图

现在我们在设计key-value存储已经讨论了不同的技术考虑，我们可以将我们的重点转移到架构图上，如图6-17所示
![img.png](/Users/rocky/study/github/SystemDesign/image/6-17.png)
系统架构的主要功能如下所列:
- 客户端通过简单的API与键值存储通信：GET（键）并放置（键，值)
- 协调器是一种充当客户端和键值存储之间的代理的节点。
- 节点使用一致性算法分布在环上。
- 系统完全是去中心化的，所以增加和删除一个节点是自动的
- 数据在多个节点复制
- 每个节点都有同一组责任，没有单点故障

由于设计分散，每个节点执行如图6-18所示的许多任务。

###### 写路径

图6-19解释了写入请求被引导到特定节点后发生的事情。 请注意，基于Cassandra的体系结构的写/读路径的建议设计[8]
1. 写请求以提交日志文件
2. 数据保存在内存缓存中
3. 当存储器缓存已满或达到预定义的阈值时，数据被刷新到磁盘上的SStable [9]。 注意：排序字符串表（SStable）是<key，value>对的排序列表。 对于有兴趣了解SSTABLE的学习的读者，请参阅参考资料[9 

###### 读路径
将读取请求定向到特定节点后，它首先检查数据是否在内存高速缓存中。 如果是这样，数据将返回给客户端，如图6-20所示。 
![img.png](/Users/rocky/study/github/SystemDesign/image/6-20.png)
图 6-20
如果数据不在内存中，则将从磁盘检索它。 我们需要一个有效的方法来找出哪个sstable包含密钥。 
绽放过滤器[10]通常用于解决这个问题 

当数据不在内存时，读取路径如图6-21所示 

1. 系统首先检查数据是否在内存中。 如果没有，请转到步骤2 
2. 如果数据不在内存中，系统会检查布隆过滤器
3. Bloom过滤器用于弄清楚哪个SSSStables可能包含密钥
4. sssstables返回数据集的结果 
5. 数据集的结果返回给客户端

##### 总结
本章涵盖了许多概念和技术。 要刷新内存，下表总结了用于分布式键值存储的功能和相应的技术。

| 目标/问题   | 技术   | 
| :------: |:------: |
| 有存储大数据的能力 | 使用哈希一致性算法将负载分摊到每台服务器上|
|高可用的读 | 数据副本，多个数据中心的配置 |
|高可用的写 | 向量时钟来解决版本和向量冲突 |
|数据库副本 |  哈希一致性算法 |
| 持续增长的扩容能力 | 哈希一致性算饭 |
| Heterogeneity 异质性 | 哈希一致性算法 |
| 可调节的一致性 |  仲裁共识 |
| 处理短暂的失败 |  loa |
| 处理永久的失败 |  load |
| 处理数据中心中断 | 跨数据中心复制 |








